[TOC]

# 绪论

## 多项式曲线拟合

generalization: 泛化，正确分类与训练集不同的新样本的能力。

用下面的多项式拟合数据
$$
y(x,w)=\sum^{M}_{j=0}w_jx^j
$$


下面的误差函数来衡量误差
$$
E(w)=\frac{1}{2}\sum^N_{n=1}\{y(x_n,w)-t_n\}^2
$$
误差函数是关于 $w$ 的二次函数，所以对误差函数求导可得出唯一的最小值。

根均方误差（RMS）以相同的基础对比不同大小的数据集。
$$
E_{RMS}=\sqrt{2E(w^*)/N}
$$
增大数据集的规模可以减小过拟合问题。另一种方法：正则化（增加一个惩罚函数）

## 概率论

加法规则、乘法规则

贝叶斯公式：
$$
p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}
$$
全概率公式：
$$
p(X)=\sum_Yp(X|Y)p(Y)
$$


### 最大似然

误差函数(error function)是似然函数的负对数，最大化似然函数等价于最小化误差函数。



### 高斯分布（正态分布）

$N(x|\mu,\sigma^2)$

$\mu$ 均值 $\sigma^2$ 方差

$\beta = \frac{1}{\sigma^2}$ 标准差

bias ：偏移

最大似然估计的平均值将会得到正确的均值，但是将会低估方差，因子为 $\frac{N-1}{N}$

MAP（最大后验）

## 维度灾难

分类问题朴素的方法: 分割单元格

如果我们把空间的 区域分割成一个个的单元格，那么这些单元格的数量会随着空间的维数以指数的形式增大。当 单元格的数量指数增大时，为了保证单元格不为空，我们就不得不需要指数量级的训练数据。

## 决策论

最小化错误分类 如果我们 把每个x分配到后验概率p(Ck | x)最大的类别中，那么我们分类错误的概率就会最小

最小化期望损失 给两类错误加上不同的权值，体现在损失函数内

拒绝选项 拒绝分类难以分类的选项，加入一个阙值

## 信息论

熵：信息量度量 单位nat





# 概率分布

## 二项分布

## Beta分布

## 多项式分布

## 狄利克雷分布

## 高斯分布

## 指数族分布

## 非参数化方法



# 回归的线性模型

## 线性基函数模型

## 偏置-方差分解

## 贝叶斯线性回归

## 贝叶斯模型比较

## 证据近似

## 固定基函数的局限性



# 分类的线性模型

如果数据集可以被线性决策面精确地分类，那么我们说这个数据集是线性可分的。

## 判别函数

判别函数是一个以向量x为输入，把它分配到K 个类别中的某一个类别(记作Ck )的函数

### Fisher判别函数

让类均值的 投影分开得较大，同时让每个类别内部的方差较小

### 感知机

## 概率生成式模型

## 概率判别式模型

## 拉普拉斯近似

## 贝叶斯logistic回归



# 神经网络

## 前馈神经网络

## 网络训练

## 误差反向传播

## Hessian矩阵

## 正经网络的正则化

## 混合密度网络

## 贝叶斯神经网络

